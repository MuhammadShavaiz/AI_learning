{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgovSYpCnyx1AhBqPghZ+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadShavaiz/AI_learning/blob/main/Sentence_Boundary_Detection_revision_needed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download the inaugural speech dataset\n",
        "nltk.download('inaugural')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49LWyRnMN5Tw",
        "outputId": "44fa45ee-9d93-47c8-bb88-2a10e1b179ea"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text data\n",
        "text_data = inaugural.raw('2009-Obama.txt')\n",
        "\n",
        "# Create a mapping from characters to integers\n",
        "chars = sorted(list(set(text_data)))\n",
        "char_to_int = {c: i for i, c in enumerate(chars)}\n",
        "num_classes = len(chars)\n",
        "\n",
        "# Create sequences and labels\n",
        "def create_sequences(text, seq_length=100):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        seq = text[i:i + seq_length]\n",
        "        label = 1 if text[i + seq_length] in '.!?;' else 0  # Sentence-ending punctuation\n",
        "        sequences.append(seq)\n",
        "        labels.append(label)\n",
        "    return sequences, labels\n",
        "\n",
        "sequences, labels = create_sequences(text_data)\n",
        "\n",
        "# One-hot encode the sequences\n",
        "def one_hot_encode(sequences, char_to_int, num_classes):\n",
        "    one_hot_encoded = []\n",
        "    for seq in sequences:\n",
        "        one_hot_seq = np.zeros((len(seq), num_classes))\n",
        "        for j, char in enumerate(seq):\n",
        "            one_hot_seq[j, char_to_int[char]] = 1\n",
        "        one_hot_encoded.append(one_hot_seq)\n",
        "    return np.array(one_hot_encoded)\n",
        "\n",
        "X = one_hot_encode(sequences, char_to_int, num_classes)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X = X.reshape((X.shape[0], X.shape[1], num_classes))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "5LCbq7rPN5RC"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n"
      ],
      "metadata": {
        "id": "fkgyN-kdN5Oi"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceBoundaryCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SentenceBoundaryCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_classes, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Calculate the size after convolutions and pooling\n",
        "        self.fc1_input_size = 64 * 25  # Based on observed dimensions\n",
        "        self.fc1 = nn.Linear(self.fc1_input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, num_classes, seq_length)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "lrBm-zmTN5Ls"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, criterion, and optimizer\n",
        "model = SentenceBoundaryCNN(num_classes=num_classes)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor).view(-1)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Optimize\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al-4WMTON5I2",
        "outputId": "7743294d-10f2-41a0-b656-8b4e315da272"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0656\n",
            "Epoch [20/100], Loss: 0.0924\n",
            "Epoch [30/100], Loss: 0.0739\n",
            "Epoch [40/100], Loss: 0.0582\n",
            "Epoch [50/100], Loss: 0.0603\n",
            "Epoch [60/100], Loss: 0.0580\n",
            "Epoch [70/100], Loss: 0.0583\n",
            "Epoch [80/100], Loss: 0.0579\n",
            "Epoch [90/100], Loss: 0.0579\n",
            "Epoch [100/100], Loss: 0.0578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test_tensor).view(-1)\n",
        "    y_pred_labels = (y_pred > 0.5).float()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (y_pred_labels == y_test_tensor).float().mean()\n",
        "print(f'Test Accuracy: {accuracy.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adcGx0Z9N5GU",
        "outputId": "1c80548f-0b72-49d1-c911-87893f6d2dfb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRn1sArTPUjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}