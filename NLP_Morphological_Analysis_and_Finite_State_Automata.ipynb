{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP0bUEbwbUE8lGUSgirvKo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadShavaiz/AI_learning/blob/main/NLP_Morphological_Analysis_and_Finite_State_Automata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Morphological Analysis and Finite State Automata**\n",
        "**This notebook explores the transformation of English nouns and verbs into their plural and past tense forms using a finite state automaton (FSA). It applies regular and irregular morphological rules while categorizing the transformed words. Additionally, the notebook incorporates spaCy for detailed derivational morphology analysis, identifying word roots, suffixes, and parts of speech. The semantic role labeling further enriches the understanding of linguistic structure and meaning, highlighting the dynamic nature of word forms and their functions in sentences.**"
      ],
      "metadata": {
        "id": "CWIovt5LxbOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noun Pluralization and Verb Tense Transformation\n",
        "\n",
        "This code transforms English nouns into plurals and verbs into past tense, addressing regular and irregular cases. It processes lists based on word endings and predefined irregular forms, categorizing the results for clarity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HNP5CeN2dB-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLc4mfH5bSyh",
        "outputId": "ae839570-953f-4f62-d322-fbe9bbad3237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular Plurals: ['cats', 'dogs', 'books', 'cars', 'cities', 'babies', 'berries', 'ladies', 'puppies', 'kittens', 'sheeps', 'lilies', 'photos', 'families', 'parties', 'keys', 'toys', 'stories', 'pianos', 'tomatoes', 'potatoes', 'memos', 'photos', 'zeros']\n",
            "Irregular Plurals: ['boxes', 'children', 'men', 'women', 'buses', 'fishes', 'mice', 'feet', 'leaves', 'geese', 'teeth', 'cacti', 'churches', 'dishes', 'classes', 'cacti']\n",
            "Regular Past Tense: ['walked', 'jumped', 'liked', 'talked', 'played', 'cried', 'tried', 'studied', 'worked', 'visited', 'enjoyed', 'asked', 'watched', 'listened', 'hurried', 'decided', 'created', 'painted', 'smiled', 'called', 'helped', 'loved', 'fixed', 'climbed', 'danced', 'arrived', 'cried', 'washed', 'carried', 'filled', 'started', 'finished', 'answered', 'believed', 'changed', 'looked', 'decided', 'needed', 'wanted', 'opened', 'closed', 'baked', 'jumped', 'painted', 'discovered']\n",
            "Irregular Past Tense: ['ran', 'had', 'leapt', 'went', 'was']\n"
          ]
        }
      ],
      "source": [
        "# Define the regular noun and verb rules\n",
        "regular_rules = {\n",
        "    'nouns': {\n",
        "        's': lambda word: word + 's',  # Default rule for regular plurals\n",
        "        'es': lambda word: word + 'es',  # Rule for nouns ending in s, x, z, sh, or ch\n",
        "        'ies': lambda word: word[:-1] + 'ies',  # Rule for nouns ending in consonant + y\n",
        "        'o_es': lambda word: word + 'es'\n",
        "    },\n",
        "    'verbs': {\n",
        "        'ed': lambda word: word + 'ed',  # Default rule for regular past tense\n",
        "        'ied': lambda word: word[:-1] + 'ied',  # Rule for verbs ending in consonant + y\n",
        "        'd': lambda word: word + 'd'  # Rule for verbs ending in e\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define irregular plural and past tense forms\n",
        "irregular_noun_forms = {\n",
        "    'child': 'children',\n",
        "    'foot': 'feet',\n",
        "    'mouse': 'mice',\n",
        "    'man': 'men',\n",
        "    'woman':'women',\n",
        "    'lead':'leaves',\n",
        "    'goose':'geese',\n",
        "    'tooth':'teeth',\n",
        "    'cactus':'cacti',\n",
        "    'leaf':'leaves'\n",
        "}\n",
        "\n",
        "irregular_verb_forms = {\n",
        "    'go': 'went',\n",
        "    'be': 'was',\n",
        "    'leap': 'leapt',\n",
        "    'have': 'had',\n",
        "    'run':'ran'\n",
        "}\n",
        "# Nouns that take 'es' when pluralized and exceptions\n",
        "nouns_ending_with_o = ['tomato', 'potato', 'hero', 'cargo', 'mango', 'volcano', 'buffalo', 'memento', 'tornado']\n",
        "\n",
        "# Function to categorize nouns and verbs into regular/irregular forms\n",
        "def categorize_words(nouns, verbs):\n",
        "    regular_plurals = []\n",
        "    irregular_plurals = []\n",
        "    regular_past_tense = []\n",
        "    irregular_past_tense = []\n",
        "\n",
        "    # Process nouns\n",
        "    for noun in nouns:\n",
        "        if noun in irregular_noun_forms:\n",
        "            irregular_plurals.append(irregular_noun_forms[noun])\n",
        "        elif noun.endswith(('s', 'x', 'z', 'sh', 'ch')):\n",
        "            irregular_plurals.append(regular_rules['nouns']['es'](noun))\n",
        "        elif noun.endswith('y') and noun[-2] not in 'aeiou':\n",
        "            regular_plurals.append(regular_rules['nouns']['ies'](noun))\n",
        "        elif noun.endswith('o'):\n",
        "            # Rule for nouns ending with 'o'\n",
        "            if noun in nouns_ending_with_o:\n",
        "                regular_plurals.append(regular_rules['nouns']['es'](noun))  # Take 'es' for specific nouns\n",
        "            else:\n",
        "                regular_plurals.append(regular_rules['nouns']['s'](noun))  # Otherwise just add 's'\n",
        "        else:\n",
        "            regular_plurals.append(regular_rules['nouns']['s'](noun))\n",
        "\n",
        "    # Process verbs\n",
        "    for verb in verbs:\n",
        "        if verb in irregular_verb_forms:\n",
        "            irregular_past_tense.append(irregular_verb_forms[verb])\n",
        "        elif verb.endswith('y') and verb[-2] not in 'aeiou':\n",
        "            regular_past_tense.append(regular_rules['verbs']['ied'](verb))\n",
        "        elif verb.endswith('e'):\n",
        "            regular_past_tense.append(regular_rules['verbs']['d'](verb))\n",
        "        else:\n",
        "            regular_past_tense.append(regular_rules['verbs']['ed'](verb))\n",
        "\n",
        "    return {\n",
        "        'Regular Plurals': regular_plurals,\n",
        "        'Irregular Plurals': irregular_plurals,\n",
        "        'Regular Past Tense': regular_past_tense,\n",
        "        'Irregular Past Tense': irregular_past_tense\n",
        "    }\n",
        "\n",
        "# Sample input: a larger list of nouns and verbs\n",
        "nouns = [\n",
        "    'cat', 'dog', 'book', 'box', 'child', 'man', 'woman', 'car',\n",
        "    'bus', 'city', 'baby', 'berry', 'lady', 'puppy', 'kitten',\n",
        "    'fish', 'sheep', 'mouse', 'foot', 'leaf', 'goose', 'tooth',\n",
        "    'cactus', 'lily', 'photo', 'church', 'dish',\n",
        "    'class', 'family', 'party', 'key', 'toy', 'story', 'piano',\n",
        "    'tomato', 'potato', 'memo', 'photo', 'cactus', 'zero'\n",
        "]\n",
        "\n",
        "verbs = [\n",
        "    'walk', 'run', 'jump', 'like', 'talk', 'play', 'cry', 'try',\n",
        "    'study', 'work', 'visit', 'enjoy', 'ask', 'watch', 'listen',\n",
        "    'hurry', 'decide', 'create', 'paint', 'smile', 'call',\n",
        "    'help', 'love', 'fix', 'climb','have', 'leap', 'dance', 'arrive',\n",
        "    'cry', 'wash', 'carry', 'fill', 'start', 'finish',\n",
        "    'answer', 'believe', 'change', 'look', 'decide',\n",
        "    'need', 'want', 'open', 'close', 'bake', 'jump', 'paint',\n",
        "    'discover','go','be'\n",
        "]\n",
        "\n",
        "# Get the categorized results\n",
        "output = categorize_words(nouns, verbs)\n",
        "\n",
        "# Print the results\n",
        "for category, words in output.items():\n",
        "    print(f\"{category}: {words}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Word Transformation Using Finite State Automaton (FSA)\n",
        "The `WordTransformerFSA` class transforms English nouns into plural forms and verbs into past tense using a finite state automaton (FSA). It applies regular rules and handles irregular forms with predefined mappings. The class determines the appropriate transformation based on the word type. Test cases showcase the functionality for both nouns and verbs."
      ],
      "metadata": {
        "id": "fu5DugTyiwIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordTransformerFSA:\n",
        "    def __init__(self):\n",
        "        # Define the regular noun and verb rules\n",
        "        self.regular_rules = {\n",
        "            'nouns': {\n",
        "                's': lambda word: word + 's',  # Default rule for regular plurals\n",
        "                'es': lambda word: word + 'es',  # Rule for nouns ending in s, x, z, sh, or ch\n",
        "                'ies': lambda word: word[:-1] + 'ies',  # Rule for nouns ending in consonant + y\n",
        "                'o_es': lambda word: word + 'es'  # For specific nouns ending with 'o'\n",
        "            },\n",
        "            'verbs': {\n",
        "                'ed': lambda word: word + 'ed',  # Default rule for regular past tense\n",
        "                'ied': lambda word: word[:-1] + 'ied',  # Rule for verbs ending in consonant + y\n",
        "                'd': lambda word: word + 'd'  # Rule for verbs ending in e\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Define irregular plural and past tense forms\n",
        "        self.irregular_noun_forms = {\n",
        "            'child': 'children',\n",
        "            'foot': 'feet',\n",
        "            'mouse': 'mice',\n",
        "            'man': 'men',\n",
        "            'woman': 'women',\n",
        "            'leaf': 'leaves',\n",
        "            'goose': 'geese',\n",
        "            'tooth': 'teeth',\n",
        "            'cactus': 'cacti'\n",
        "        }\n",
        "\n",
        "        self.irregular_verb_forms = {\n",
        "            'go': 'went',\n",
        "            'be': 'was',\n",
        "            'leap': 'leapt',\n",
        "            'have': 'had',\n",
        "            'run': 'ran'\n",
        "        }\n",
        "\n",
        "        # Nouns that take 'es' when pluralized and exceptions\n",
        "        self.nouns_ending_with_o = ['tomato', 'potato', 'hero', 'cargo', 'mango', 'volcano', 'buffalo', 'memento', 'tornado']\n",
        "\n",
        "        # Define the states for the FSA\n",
        "        self.states = ['singular', 'plural', 'infinitive', 'past', 'irregular_plural', 'irregular_past']\n",
        "        self.final_states = ['plural', 'past', 'irregular_plural', 'irregular_past']\n",
        "\n",
        "    def transition(self, word, word_type):\n",
        "        \"\"\"\n",
        "        Simulates the FSA transitions based on the word type and its morphological rules.\n",
        "        \"\"\"\n",
        "        # Start at initial state\n",
        "        current_state = 'singular' if word_type == 'noun' else 'infinitive'\n",
        "\n",
        "        # Handle nouns\n",
        "        if word_type == 'noun':\n",
        "            if word in self.irregular_noun_forms:\n",
        "                current_state = 'irregular_plural'\n",
        "            else:\n",
        "                current_state = 'plural'\n",
        "\n",
        "        # Handle verbs\n",
        "        elif word_type == 'verb':\n",
        "            if word in self.irregular_verb_forms:\n",
        "                current_state = 'irregular_past'\n",
        "            else:\n",
        "                current_state = 'past'\n",
        "\n",
        "        return current_state\n",
        "\n",
        "    def transform(self, word, word_type):\n",
        "        \"\"\"\n",
        "        Transforms the word based on its type (noun or verb) and returns the plural or past form.\n",
        "        \"\"\"\n",
        "        # Determine the transformation state\n",
        "        current_state = self.transition(word, word_type)\n",
        "\n",
        "        # Apply noun transformation rules\n",
        "        if word_type == 'noun':\n",
        "            if current_state == 'irregular_plural':\n",
        "                return self.irregular_noun_forms[word]\n",
        "            elif current_state == 'plural':\n",
        "                if word.endswith(('s', 'x', 'z', 'sh', 'ch')):\n",
        "                    return self.regular_rules['nouns']['es'](word)\n",
        "                elif word.endswith('y') and word[-2] not in 'aeiou':\n",
        "                    return self.regular_rules['nouns']['ies'](word)\n",
        "                elif word.endswith('o'):\n",
        "                    if word in self.nouns_ending_with_o:\n",
        "                        return self.regular_rules['nouns']['es'](word)\n",
        "                    else:\n",
        "                        return self.regular_rules['nouns']['s'](word)\n",
        "                else:\n",
        "                    return self.regular_rules['nouns']['s'](word)\n",
        "\n",
        "        # Apply verb transformation rules\n",
        "        elif word_type == 'verb':\n",
        "            if current_state == 'irregular_past':\n",
        "                return self.irregular_verb_forms[word]\n",
        "            elif current_state == 'past':\n",
        "                if word.endswith('y') and word[-2] not in 'aeiou':\n",
        "                    return self.regular_rules['verbs']['ied'](word)\n",
        "                elif word.endswith('e'):\n",
        "                    return self.regular_rules['verbs']['d'](word)\n",
        "                else:\n",
        "                    return self.regular_rules['verbs']['ed'](word)\n",
        "        # Under above conditions this will not get triggered, change the rules for adding 's' for nouns and 'ed' for verbs to return unrecognized words\n",
        "        return \"Word not recognized or cannot be transformed.\"\n",
        "\n",
        "# Instantiate the FSA\n",
        "fsa = WordTransformerFSA()\n",
        "\n",
        "# Test cases\n",
        "nouns = [\n",
        "    'cat', 'dog', 'book', 'box', 'child', 'man', 'woman', 'car',\n",
        "    'bus', 'city', 'baby', 'berry', 'lady', 'puppy', 'kitten',\n",
        "    'fish', 'sheep', 'mouse', 'foot', 'leaf', 'goose', 'tooth',\n",
        "    'cactus', 'lily', 'photo', 'church', 'dish',\n",
        "    'class', 'family', 'party', 'key', 'toy', 'story', 'piano',\n",
        "    'tomato', 'potato', 'memo', 'photo', 'cactus', 'zero'\n",
        "]\n",
        "\n",
        "verbs = [\n",
        "    'walk', 'run', 'jump', 'like', 'talk', 'play', 'cry', 'try',\n",
        "    'study', 'work', 'visit', 'enjoy', 'ask', 'watch', 'listen',\n",
        "    'hurry', 'decide', 'create', 'paint', 'smile', 'call',\n",
        "    'help', 'love', 'fix', 'climb','have', 'leap', 'dance', 'arrive',\n",
        "    'cry', 'wash', 'carry', 'fill', 'start', 'finish',\n",
        "    'answer', 'believe', 'change', 'look', 'decide',\n",
        "    'need', 'want', 'open', 'close', 'bake', 'jump', 'paint',\n",
        "    'discover','go','be'\n",
        "]\n",
        "\n",
        "# Test noun transformations\n",
        "print(\"Noun Transformations:\")\n",
        "for noun in nouns:\n",
        "    print(f\"{noun} -> {fsa.transform(noun, 'noun')}\")\n",
        "\n",
        "# Test verb transformations\n",
        "print(\"\\nVerb Transformations:\")\n",
        "for verb in verbs:\n",
        "    print(f\"{verb} -> {fsa.transform(verb, 'verb')}\")\n"
      ],
      "metadata": {
        "id": "9wyM870Jdc9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4c64d9-5e6a-4c67-f128-f4418d828e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Transformations:\n",
            "cat -> cats\n",
            "dog -> dogs\n",
            "book -> books\n",
            "box -> boxes\n",
            "child -> children\n",
            "man -> men\n",
            "woman -> women\n",
            "car -> cars\n",
            "bus -> buses\n",
            "city -> cities\n",
            "baby -> babies\n",
            "berry -> berries\n",
            "lady -> ladies\n",
            "puppy -> puppies\n",
            "kitten -> kittens\n",
            "fish -> fishes\n",
            "sheep -> sheeps\n",
            "mouse -> mice\n",
            "foot -> feet\n",
            "leaf -> leaves\n",
            "goose -> geese\n",
            "tooth -> teeth\n",
            "cactus -> cacti\n",
            "lily -> lilies\n",
            "photo -> photos\n",
            "church -> churches\n",
            "dish -> dishes\n",
            "class -> classes\n",
            "family -> families\n",
            "party -> parties\n",
            "key -> keys\n",
            "toy -> toys\n",
            "story -> stories\n",
            "piano -> pianos\n",
            "tomato -> tomatoes\n",
            "potato -> potatoes\n",
            "memo -> memos\n",
            "photo -> photos\n",
            "cactus -> cacti\n",
            "zero -> zeros\n",
            "\n",
            "Verb Transformations:\n",
            "walk -> walked\n",
            "run -> ran\n",
            "jump -> jumped\n",
            "like -> liked\n",
            "talk -> talked\n",
            "play -> played\n",
            "cry -> cried\n",
            "try -> tried\n",
            "study -> studied\n",
            "work -> worked\n",
            "visit -> visited\n",
            "enjoy -> enjoyed\n",
            "ask -> asked\n",
            "watch -> watched\n",
            "listen -> listened\n",
            "hurry -> hurried\n",
            "decide -> decided\n",
            "create -> created\n",
            "paint -> painted\n",
            "smile -> smiled\n",
            "call -> called\n",
            "help -> helped\n",
            "love -> loved\n",
            "fix -> fixed\n",
            "climb -> climbed\n",
            "have -> had\n",
            "leap -> leapt\n",
            "dance -> danced\n",
            "arrive -> arrived\n",
            "cry -> cried\n",
            "wash -> washed\n",
            "carry -> carried\n",
            "fill -> filled\n",
            "start -> started\n",
            "finish -> finished\n",
            "answer -> answered\n",
            "believe -> believed\n",
            "change -> changed\n",
            "look -> looked\n",
            "decide -> decided\n",
            "need -> needed\n",
            "want -> wanted\n",
            "open -> opened\n",
            "close -> closed\n",
            "bake -> baked\n",
            "jump -> jumped\n",
            "paint -> painted\n",
            "discover -> discovered\n",
            "go -> went\n",
            "be -> was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Morphology and Semantic Role Analysis\n",
        "This script leverages spaCy to perform detailed analysis on sentences, focusing on derivational morphology by identifying word roots, suffixes, and their parts of speech. Additionally, it assigns semantic roles (such as action, object, and subject) to each token, providing insights into the linguistic structure and meaning of the sentence. This dual analysis aids in understanding how words change form and function within a sentence."
      ],
      "metadata": {
        "id": "lJkPiF3TfJwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYHVX6UQZPFc",
        "outputId": "f01772c0-4fc5-4721-e5c0-9dcb1adee254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def derivational_morphology(token):\n",
        "    # Using spaCy's lemma for the root\n",
        "    root = token.lemma_\n",
        "    suffix = ''\n",
        "    morph_pos = token.pos_  # Get the POS tag from the token\n",
        "\n",
        "    # Check for suffixes\n",
        "    suffixes = {\n",
        "        # Adjectives\n",
        "        'able': 'adjective', 'ful': 'adjective', 'ic': 'adjective', 'less': 'adjective', 'ous': 'adjective',\n",
        "        # Nouns\n",
        "        'ance': 'noun', 'hood': 'noun', 'ity': 'noun', 'ment': 'noun', 'ness': 'noun', 'ship': 'noun', 'tion': 'noun','s':'noun',\n",
        "        # Verbs\n",
        "        'ate': 'verb', 'en': 'verb', 'ify': 'verb', 'ize': 'verb', 'ed': 'verb', 'd': 'verb', 'ied': 'verb','ing':'verb',\n",
        "        # Adverbs\n",
        "        'ly': 'adverb',\n",
        "    }\n",
        "\n",
        "    # Check if the token has a suffix\n",
        "    for s in suffixes.keys():\n",
        "        if token.text.endswith(s):\n",
        "            suffix = s\n",
        "            # Update the root based on the suffix\n",
        "            root = token.text[:-len(s)]\n",
        "            morph_pos = suffixes[suffix]  # Set the POS based on the identified suffix\n",
        "            break\n",
        "\n",
        "    return root, suffix, morph_pos\n",
        "\n",
        "def semantic_role_labeling(token):\n",
        "    if token.pos_ == \"VERB\":\n",
        "        return 'action'\n",
        "    elif token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "        return 'object'\n",
        "    elif token.pos_ == \"ADJ\":\n",
        "        return 'adjective'\n",
        "    elif token.pos_ == \"ADV\":\n",
        "        return 'adverb'\n",
        "    elif token.pos_ in [\"PRON\", \"DET\"]:  # pronouns and determiners\n",
        "        return 'subject'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "def analyze_sentence(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    print(\"Derivational Morphology Analysis:\")\n",
        "    for token in doc:\n",
        "        root, suffix, morph_pos = derivational_morphology(token)\n",
        "        print(f\"{token.text} -> Root: {root}, Suffix: {suffix}, Part of Speech: {morph_pos}\")\n",
        "\n",
        "    print(\"\\nSemantic Role Labeling:\")\n",
        "    for token in doc:\n",
        "        role = semantic_role_labeling(token)\n",
        "        print(f\"{token.text} -> Role: {role}\")\n",
        "\n",
        "# Sample Input\n",
        "sentence = \"The enthusiastic professor swiftly instructs the pupils. She loves educating.\"\n",
        "analyze_sentence(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqwfXZMPiIom",
        "outputId": "68120a7c-b6f5-47f9-aeb9-e92d835b7974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Derivational Morphology Analysis:\n",
            "The -> Root: the, Suffix: , Part of Speech: DET\n",
            "enthusiastic -> Root: enthusiast, Suffix: ic, Part of Speech: adjective\n",
            "professor -> Root: professor, Suffix: , Part of Speech: NOUN\n",
            "swiftly -> Root: swift, Suffix: ly, Part of Speech: adverb\n",
            "instructs -> Root: instruct, Suffix: s, Part of Speech: noun\n",
            "the -> Root: the, Suffix: , Part of Speech: DET\n",
            "pupils -> Root: pupil, Suffix: s, Part of Speech: noun\n",
            ". -> Root: ., Suffix: , Part of Speech: PUNCT\n",
            "She -> Root: she, Suffix: , Part of Speech: PRON\n",
            "loves -> Root: love, Suffix: s, Part of Speech: noun\n",
            "educating -> Root: educat, Suffix: ing, Part of Speech: verb\n",
            ". -> Root: ., Suffix: , Part of Speech: PUNCT\n",
            "\n",
            "Semantic Role Labeling:\n",
            "The -> Role: subject\n",
            "enthusiastic -> Role: adjective\n",
            "professor -> Role: object\n",
            "swiftly -> Role: adverb\n",
            "instructs -> Role: action\n",
            "the -> Role: subject\n",
            "pupils -> Role: object\n",
            ". -> Role: unknown\n",
            "She -> Role: subject\n",
            "loves -> Role: action\n",
            "educating -> Role: action\n",
            ". -> Role: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xM5HuobEu6WE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}